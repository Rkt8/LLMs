{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09386d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9042c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    GPT2TokenizerFast,    # BPE\n",
    "    BertTokenizerFast,    # WordPiece  \n",
    "    XLNetTokenizerFast,   # Unigram\n",
    "    T5TokenizerFast       # SentencePiece + Unigram\n",
    ")\n",
    "import numpy as np\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4817f041",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_test = [\n",
    "    \"El preprocessing es crucial en NLP moderno\",\n",
    "    \"Evaluemos corre, corro, corremos, corrieron\",\n",
    "    \"Neuroplasticity enables continuous learning adaptation\", \n",
    "    'Palabras complejas como ionización, ánodo, sulfhídrico o comsmología puede que sean más difíciles de decifrar'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8cffed91",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers = {\n",
    "    'BPE (GPT-2)': GPT2TokenizerFast.from_pretrained('gpt2'),\n",
    "    'WordPiece (BERT)': BertTokenizerFast.from_pretrained('bert-base-uncased'),\n",
    "    'Unigram (XLNet)': XLNetTokenizerFast.from_pretrained('xlnet-base-cased'),\n",
    "    'SentencePiece (T5)': T5TokenizerFast.from_pretrained('t5-small')\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1417b407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Palabra: El preprocessing es crucial en NLP moderno\n",
      "  BPE (GPT-2)    : 10 tokens → ['El', 'Ġpre', 'processing', 'Ġes', 'Ġcrucial', 'Ġen', 'ĠN', 'LP', 'Ġmodern', 'o']\n",
      "  WordPiece (BERT): 12 tokens → ['el', 'prep', '##ro', '##ces', '##sing', 'es', 'crucial', 'en', 'nl', '##p', 'modern', '##o']\n",
      "  Unigram (XLNet): 12 tokens → ['▁El', '▁pre', 'processing', '▁', 'es', '▁crucial', '▁', 'en', '▁N', 'LP', '▁modern', 'o']\n",
      "  SentencePiece (T5): 14 tokens → ['▁El', '▁pre', 'process', 'ing', '▁', 'e', 's', '▁crucial', '▁', 'en', '▁N', 'LP', '▁modern', 'o']\n",
      "\n",
      "Palabra: Evaluemos corre, corro, corremos, corrieron\n",
      "  BPE (GPT-2)    : 17 tokens → ['E', 'val', 'u', 'em', 'os', 'Ġcor', 're', ',', 'Ġcorro', ',', 'Ġcor', 'rem', 'os', ',', 'Ġcor', 'rier', 'on']\n",
      "  WordPiece (BERT): 17 tokens → ['eva', '##lu', '##em', '##os', 'co', '##rre', ',', 'co', '##rro', ',', 'co', '##rre', '##mos', ',', 'co', '##rrier', '##on']\n",
      "  Unigram (XLNet): 20 tokens → ['▁E', 'value', 'mos', '▁', 'cor', 're', ',', '▁', 'cor', 'ro', ',', '▁', 'cor', 're', 'mos', ',', '▁', 'cor', 'rier', 'on']\n",
      "  SentencePiece (T5): 16 tokens → ['▁E', 'value', 'mos', '▁cor', 're', ',', '▁', 'corro', ',', '▁cor', 're', 'mos', ',', '▁cor', 'rier', 'on']\n",
      "\n",
      "Palabra: Neuroplasticity enables continuous learning adaptation\n",
      "  BPE (GPT-2)    :  9 tokens → ['Ne', 'uro', 'pl', 'astic', 'ity', 'Ġenables', 'Ġcontinuous', 'Ġlearning', 'Ġadaptation']\n",
      "  WordPiece (BERT):  9 tokens → ['ne', '##uro', '##pl', '##astic', '##ity', 'enables', 'continuous', 'learning', 'adaptation']\n",
      "  Unigram (XLNet):  7 tokens → ['▁Neuro', 'plastic', 'ity', '▁enables', '▁continuous', '▁learning', '▁adaptation']\n",
      "  SentencePiece (T5):  8 tokens → ['▁Neuro', 'plastic', 'ity', '▁', 'enables', '▁continuous', '▁learning', '▁adaptation']\n",
      "\n",
      "Palabra: Palabras complejas como ionización, ánodo, sulfhídrico o comsmología puede que sean más difíciles de decifrar\n",
      "  BPE (GPT-2)    : 45 tokens → ['Pal', 'ab', 'ras', 'Ġcomple', 'jas', 'Ġcom', 'o', 'Ġion', 'iz', 'aci', 'Ã³n', ',', 'ĠÃ', '¡', 'n', 'odo', ',', 'Ġsulf', 'h', 'ÃŃ', 'd', 'ric', 'o', 'Ġo', 'Ġcom', 'sm', 'olog', 'ÃŃa', 'Ġp', 'ued', 'e', 'Ġque', 'Ġse', 'an', 'Ġm', 'Ã¡s', 'Ġd', 'if', 'ÃŃ', 'cil', 'es', 'Ġde', 'Ġdec', 'if', 'rar']\n",
      "  WordPiece (BERT): 38 tokens → ['pal', '##ab', '##ras', 'com', '##ple', '##jas', 'como', 'ion', '##iza', '##cion', ',', 'an', '##od', '##o', ',', 'sul', '##f', '##hid', '##ric', '##o', 'o', 'com', '##smo', '##log', '##ia', 'pu', '##ede', 'que', 'sean', 'mas', 'di', '##fi', '##ci', '##les', 'de', 'dec', '##if', '##rar']\n",
      "  Unigram (XLNet): 50 tokens → ['▁Pala', 'bra', 's', '▁comp', 'le', 'ja', 's', '▁com', 'o', '▁', 'ion', 'iz', 'acion', ',', '▁an', 'odo', ',', '▁', 's', 'ulf', 'hid', 'rico', '▁', 'o', '▁com', 's', 'm', 'ologi', 'a', '▁', 'pu', 'ed', 'e', '▁', 'que', '▁', 's', 'ean', '▁', 'mas', '▁', 'd', 'ific', 'ile', 's', '▁de', '▁de', 'ci', 'fra', 'r']\n",
      "  SentencePiece (T5): 60 tokens → ['▁Pala', 'bra', 's', '▁comp', 'le', 'ja', 's', '▁', 'com', 'o', '▁', 'i', 'on', 'iza', 'ción', ',', '▁', 'án', 'o', 'd', 'o', ',', '▁', 's', 'ul', 'f', 'h', 'í', 'd', 'ric', 'o', '▁', 'o', '▁', 'com', 's', 'm', 'o', 'log', 'í', 'a', '▁pu', 'e', 'de', '▁que', '▁se', 'an', '▁', 'más', '▁', 'd', 'if', 'í', 'c', 'ile', 's', '▁de', '▁de', 'cifra', 'r']\n"
     ]
    }
   ],
   "source": [
    "for palabra in corpus_test:\n",
    "    print(f\"\\nPalabra: {palabra}\")\n",
    "    for name, tokenizer in tokenizers.items():\n",
    "        tokens = tokenizer.tokenize(palabra)\n",
    "        print(f\"  {name:<15}: {len(tokens):2d} tokens → {tokens}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
